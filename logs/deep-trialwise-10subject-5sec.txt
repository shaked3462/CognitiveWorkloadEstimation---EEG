INFO : deep Model, 100 Epoches, 8 Batch Size, 10 Subjects
loading data for subject 1
INFO : label 0 examples: 64, label 1 examples: 201, label 2 examples: 345
INFO : used 64 examples from each label
(610, 62, 1281)
loading data for subject 2
INFO : label 0 examples: 42, label 1 examples: 57, label 2 examples: 117
INFO : used 42 examples from each label
loading data for subject 3
INFO : label 0 examples: 44, label 1 examples: 79, label 2 examples: 194
INFO : used 44 examples from each label
loading data for subject 4
INFO : label 0 examples: 69, label 1 examples: 126, label 2 examples: 187
INFO : used 69 examples from each label
loading data for subject 5
INFO : label 0 examples: 18, label 1 examples: 16, label 2 examples: 19
INFO : used 16 examples from each label
loading data for subject 6
INFO : label 0 examples: 141, label 1 examples: 264, label 2 examples: 260
INFO : used 141 examples from each label
loading data for subject 7
INFO : label 0 examples: 53, label 1 examples: 81, label 2 examples: 98
INFO : used 53 examples from each label
loading data for subject 8
INFO : label 0 examples: 28, label 1 examples: 33, label 2 examples: 112
INFO : used 28 examples from each label
loading data for subject 9
INFO : label 0 examples: 42, label 1 examples: 112, label 2 examples: 240
INFO : used 42 examples from each label
loading data for subject 10
INFO : label 0 examples: 17, label 1 examples: 32, label 2 examples: 34
INFO : used 17 examples from each label
num of examples for each label 518
trying to run with 2.5 sec trials
y
[0 2 1 1 1 2 0 0 2 0 2 2 0 1 1 0 0 1 2 0 1 2 1 1 0 1 0 1 0 0 1 0 1 2 2 1 2
 0 0 2 0 1 2 0 1 1 2 1 0 0 0 1 0 1 1 0 2 1 1 0 1 2 0 0 0 1 0 2 0 0 0 2 0 1
 2 2 1 0 2 2 0 0 0 1 1 0 1 0 2 1 0 0 2 0 1 0 1 1 2 1 1 2 0 0 2 0 0 0 1 1 2
 0 2 2 2 1 2 1 0 2 0 0 0 0 0 2 0 2 0 0 0 2 1 0 2 2 1 1 2 0 0 0 1 2 2 1 1 1
 2 1 0 1 1 0 0 1 0 1 0 2 1 2 1 2 1 2 0 2 1 2 1 2 1 1 1 1 2 1 1 1 0 2 1 2 0
 2 0 1 2 2 0 2 1 0 2 2 1 0 0 0 2 2 2 0 1 2 2 1 1 1 1 1 2 1 0 0 0 2 2 0 2 1
 2 1 2 2 1 2 2 0 0 1 1 0 0 2 0 0 2 1 1 0 1 2 1 1 1 2 0 2 2 2 1 1 2 2 0 0 2
 1 2 0 1 2 0 2 2 1 0 0 2 1 2 0 1 1 2 0 0 0 0 1 2 1 0 1 2 1 0 1 0 1 0 2 1 2
 1 0 1 0 2 2 0 2 0 2 0 0 1 1 0 1 0 1 2 0 1 1 1 2 0 2 1 1 1 2 0 0 2 1 0 1 0
 2 0 2 2 1 2 2 0 0 2 2 1 1 1 2 1 0 2 2 2 0 1 0 1 2 0 0 1 2 0 1 2 0 2 1 2 2
 2 0 1 2 0 2 1 1 1 1 2 2 0 0 1 1 2 0 1 2 0 1 1 0 0 0 2 0 0 2 2 1 0 2 2 2 2
 1 2 1 2 2 1 1 1 1 1 1 1 2 0 1 2 2 2 0 2 0 1 0 1 1 0 2 2 0 0 1 2 1 1 2 0 2
 1 0 0 0 0 2 0 1 1 1 0 2 1 1 2 2 0 0 0 1 2 2 1 2 2 1 0 1 0 2 2 1 1 2 0 0 2
 1 1 0 0 2 1 2 0 1 2 2 2 1 2 0 0 2 1 1 0 2 2 0 0 0 2 2 2 2 2 1 1 2 0 0 0 2
 0 0 2 1 2 1 1 0 1 0 2 0 1 2 1 1 1 2 2 1 1 1 0 2 2 2 0 1 0 1 1 0 2 2 0 2 2
 2 2 0 0 0 2 1 2 1 2 1 0 1 0 0 1 0 1 0 0 2 0 1 0 1 0 1 2 0 0 1 2 2 0 1 2 2
 2 0 1 0 2 0 1 1 1 1 0 2 0 2 2 1 0 1 0 2 2 1 1 2 0 1 0 0 1 2 1 1 1 1 1 1 1
 2 0 1 1 0 1 1 0 2 0 0 1 2 2 1 2 2 1 1 0 2 1 1 0 2 1 0 0 0 2 1 1 2 1 1 0 2
 1 1 2 0 0 1 0 2 1 0 2 0 1 0 1 2 1 2 0 1 0 1 2 2 0 1 0 1 2 0 0 0 1 1 2 0 2
 1 0 1 1 0 2 2 1 0 2 1 1 1 2 0 1 2 1 0 1 1 0 0 1 1 1 1 2 2 0 1 1 1 0 0 1 1
 1 1 1 2 2 1 1 2 2 1 0 1 0 1 0 0 1 1 1 1 2 1 2 2 2 0 2 0 2 2 0 2 2 2 2 1 1
 1 2 1 2 2 2 0 2 2 0 2 1 0 1 0 2 2 0 1 2 2 0 0 1 0 2 2 1 2 0 1 0 1 1 2 0 1
 0 1 2 0 0 2 0 1 1 2 2 2 2 0 2 0 1 0 0 1 1 2 1 2 0 0 1 2 2 2 1 1 1 2 0 2 1
 2 2 1 2 2 0 0 0 1 0 2 1 0 2 0 0 0 1 2 2 1 0 0 0 0 1 2 2 0 0 0 2 2 0 1 1 2
 2 0 2 0 1 2 2 1 2 2 0 0 1 1 2 1 0 0 1 2 0 1 1 0 2 2 1 2 0 2 1 0 1 0 2 0 0
 2 1 0 2 0 1 1 1 1 2 1 0 0 2 1 0 0 1 0 1 1 2 1 2 0 1 2 1 1 1 0 0 2 2 0 0 2
 2 2 2 1 2 2 0 0 2 1 1 1 0 2 0 1 2 2 0 1 1 2 2 2 0 1 1 0 0 2 2 0 0 1 0 2 2
 1 0 2 0 0 2 1 0 2 2 0 0 2 0 2 0 1 2 2 2 2 1 1 0 1 1 1 2 2 1 1 2 0 1 0 2 2
 1 2 2 0 2 1 2 0 0 2 1 1 1 2 0 1 0 0 0 1 1 1 0 0 2 0 0 0 1 2 2 1 2 2 1 2 1
 2 2 0 2 1 0 0 2 2 2 1 0 2 1 2 1 0 2 2 2 1 2 0 1 2 2 1 0 2 0 2 2 1 2 1 0 2
 0 1 1 2 1 2 2 1 2 1 0 2 2 1 1 1 1 0 1 2 0 2 0 2 1 1 1 2 2 0 0 0 0 2 2 2 1
 1 0 0 2 2 1 1 2 2 0 0 2 1 1 1 1 0 2 0 1 2 0 1 1 2 0 2 0 0 1 1 2 2 1 0 0 0
 0 1 2 0 1 0 1 1 0 1 0 2 1 0 2 2 1 1 0 1 2 1 0 1 1 2 2 1 1 1 1 2 1 2 0 1 0
 2 2 0 1 0 2 2 0 0 1 2 0 1 1 2 2 0 1 2 2 0 0 2 1 0 1 0 0 0 1 0 1 1 0 1 2 2
 0 2 0 0 2 2 2 1 0 0 0 2 2 0 2 0 2 0 2 0 2 1 0 1 0 0 1 0 2 0 0 0 2 0 0 1 1
 1 0 2 2 2 1 0 0 1 2 2 1 2 1 1 1 1 0 2 1 1 1 1 1 2 0 1 0 0 1 2 0 2 1 0 1 0
 2 2 1 2 0 1 0 0 2 0 2 1 1 2 0 2 0 0 2 0 2 1 2 0 1 0 0 0 1 1 2 1 0 2 1 0 2
 1 1 0 1 2 2 2 2 2 1 0 2 0 2 0 1 2 0 2 0 2 2 1 2 0 0 1 1 2 1 1 2 0 1 0 2 2
 0 1 0 2 2 2 1 0 0 2 1 2 1 1 0 0 2 0 0 1 2 1 1 0 2 0 0 0 0 1 0 0 2 1 1 2 1
 0 2 2 1 1 0 2 0 2 1 0 2 2 2 0 1 0 2 1 2 2 1 0 0 1 1 1 0 2 1 2 0 0 0 1 0 0
 0 0 0 1 0 2 1 0 2 0 0 2 2 0 0 1 0 2 0 0 0 2 1 0 2 0 2 1 2 1 1 2 0 0 1 2 0
 1 0 0 2 2 0 1 0 0 2 2 1 0 1 2 0 1 2 1 2 0 0 2 2 0 0 0 1 2 1 0 0 0 2 2 0 0]
INFO : Training sample size: 932
INFO : Validation sample size: 310
INFO : Test sample size: 310
2019-06-02 19:57:54,704 INFO : Model: 
Sequential(
  (dimshuffle): Expression(expression=_transpose_time_to_spat)
  (conv_time): Conv2d(1, 25, kernel_size=(10, 1), stride=(1, 1))
  (conv_spat): Conv2d(25, 25, kernel_size=(1, 62), stride=(1, 1), bias=False)
  (bnorm): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv_nonlin): Expression(expression=elu)
  (pool): MaxPool2d(kernel_size=(3, 1), stride=(3, 1), padding=0, dilation=1, ceil_mode=False)
  (pool_nonlin): Expression(expression=identity)
  (drop_2): Dropout(p=0.5)
  (conv_2): Conv2d(25, 50, kernel_size=(10, 1), stride=(1, 1), bias=False)
  (bnorm_2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (nonlin_2): Expression(expression=elu)
  (pool_2): MaxPool2d(kernel_size=(3, 1), stride=(3, 1), padding=0, dilation=1, ceil_mode=False)
  (pool_nonlin_2): Expression(expression=identity)
  (drop_3): Dropout(p=0.5)
  (conv_3): Conv2d(50, 100, kernel_size=(10, 1), stride=(1, 1), bias=False)
  (bnorm_3): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (nonlin_3): Expression(expression=elu)
  (pool_3): MaxPool2d(kernel_size=(3, 1), stride=(3, 1), padding=0, dilation=1, ceil_mode=False)
  (pool_nonlin_3): Expression(expression=identity)
  (drop_4): Dropout(p=0.5)
  (conv_4): Conv2d(100, 200, kernel_size=(10, 1), stride=(1, 1), bias=False)
  (bnorm_4): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (nonlin_4): Expression(expression=elu)
  (pool_4): MaxPool2d(kernel_size=(3, 1), stride=(3, 1), padding=0, dilation=1, ceil_mode=False)
  (pool_nonlin_4): Expression(expression=identity)
  (conv_classifier): Conv2d(200, 3, kernel_size=(11, 1), stride=(1, 1))
  (softmax): LogSoftmax()
  (squeeze): Expression(expression=_squeeze_final_output)
)
2019-06-02 19:57:54,704 INFO : Run until first stop...
2019-06-02 19:58:25,028 INFO : Epoch 0
2019-06-02 19:58:25,028 INFO : train_loss                3.03501
2019-06-02 19:58:25,028 INFO : valid_loss                2.74592
2019-06-02 19:58:25,028 INFO : test_loss                 3.38732
2019-06-02 19:58:25,028 INFO : train_misclass            0.64056
2019-06-02 19:58:25,028 INFO : valid_misclass            0.65161
2019-06-02 19:58:25,028 INFO : test_misclass             0.69872
2019-06-02 19:58:25,028 INFO : runtime                   0.00000
2019-06-02 19:58:25,028 INFO : 
2019-06-02 19:58:25,028 INFO : New best valid_misclass: 0.651613
2019-06-02 19:58:25,028 INFO : 
